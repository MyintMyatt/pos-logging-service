# logstash-kafka.conf
input {
  kafka {
#     bootstrap_servers => "192.168.240.52:9093,192.168.240.34:9093,192.168.245.118:9093"
    bootstrap_servers => "kafka:29092"
    topics_pattern => "-.*-logs" # subscribe to All service logs topics
   # topics => ["logs-topic"] # Must match 'logging.kafka.topic'
    group_id => "logs-consumer-group"
    auto_offset_reset => "earliest"
    codec => "json" # This is key! It parses the JSON logs sent by Spring Boot.


    # kafka security
    # security_protocol => "SASL_SSL"
    # sasl_mechanism => "SCRAM-SHA-512"

    #sasl_jaas_config => "org.apache.kafka.common.security.scram.ScramLoginModule required username=\"logstash\" password=\"logstash@2025\";"
    # sasl_jaas_config => 'org.apache.kafka.common.security.scram.ScramLoginModule required username="logstash" password="logstash@2025";'
    # ssl_truststore_location => "/etc/logstash/security/client.truststore.jks"
    # ssl_truststore_type => "JKS"
    # ssl_truststore_password => "password"

    # ssl_keystore_location => "/etc/logstash/security/client.keystore.jks"
    # ssl_keystore_password => "password"
    # ssl_keystore_type => "JKS"
    # ssl_key_password => "password"

    # ssl_endpoint_identification_algorithm => ""
  }
}

filter {
  # No complex filtering needed since the log is alr    eady structured JSON.
  # Use this section only for enrichment or normalization if required.
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "password"
    ssl_verification_mode => "none"
    data_stream => false
    index => "%{[@metadata][kafka][topic]}-%{+YYYY.MM.dd}"
    # index => "mail-service-logs-%{+YYYY.MM.dd}"
    # Use the @timestamp field from the JSON log as the event timestamp
    manage_template => false
  }
}