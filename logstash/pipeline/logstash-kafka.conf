# logstash-kafka.conf
input {
  kafka {
    bootstrap_servers => "192.168.240.52:9093,192.168.240.34:9093,192.168.245.118:9093"
  #  bootstrap_servers => "kafka:29092"
    topics_pattern => ".*-logs" # subscribe to All service logs topics
   # topics => ["logs-topic"] # Must match 'logging.kafka.topic'
    group_id => "logs-consumer-group"
    auto_offset_reset => "earliest"
    codec => "json" # This is key! It parses the JSON logs sent by Spring Boot.
    decorate_events => true


    # kafka security
    security_protocol => "SASL_SSL"
    sasl_mechanism => "SCRAM-SHA-512"

    #sasl_jaas_config => "org.apache.kafka.common.security.scram.ScramLoginModule required username=\"logstash\" password=\"logstash@2025\";"
    sasl_jaas_config => 'org.apache.kafka.common.security.scram.ScramLoginModule required username="logstash" password="logstash@2025";'
    ssl_truststore_location => "/etc/logstash/security/client.truststore.jks"
    ssl_truststore_type => "JKS"
    ssl_truststore_password => "password"

    ssl_keystore_location => "/etc/logstash/security/client.keystore.jks"
    ssl_keystore_password => "password"
    ssl_keystore_type => "JKS"
    ssl_key_password => "password"

    ssl_endpoint_identification_algorithm => ""
  }
}

filter {
    grok {
        match => {
        "[@metadata][kafka][topic]" => "%{WORD:env}\.%{GREEDYDATA:service}-logs"
        }
    }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "password"
    ssl_verification_mode => "none"

  # index config
  #  data_stream => false
  #  index => "%{[@metadata][kafka][topic]}-%{+YYYY.MM.dd}"

  # data_stream config
    data_stream => true
    data_stream_type => "logs"
    data_stream_dataset =>  "%{[service]}"
    data_stream_namespace => "%{[env]}"

#     manage_template => true
  }
   stdout { codec => rubydebug }
}