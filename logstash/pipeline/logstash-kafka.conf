# logstash-kafka.conf
input {
  kafka {
    bootstrap_servers => "192.168.240.52:9093,192.168.240.34:9093,192.168.245.118:9093"
  #  bootstrap_servers => "kafka:29092"
    topics_pattern => ".*-logs" # subscribe to All service logs topics
   # topics => ["logs-topic"] # Must match 'logging.kafka.topic'
    group_id => "logs-consumer-group"
    auto_offset_reset => "earliest"
    codec => "json" # This is key! It parses the JSON logs sent by Spring Boot.
    decorate_events => true


    # kafka security
    security_protocol => "SASL_SSL"
    sasl_mechanism => "SCRAM-SHA-512"

    #sasl_jaas_config => "org.apache.kafka.common.security.scram.ScramLoginModule required username=\"logstash\" password=\"logstash@2025\";"
    sasl_jaas_config => 'org.apache.kafka.common.security.scram.ScramLoginModule required username="logstash" password="logstash@2025";'
    ssl_truststore_location => "/etc/logstash/security/client.truststore.jks"
    ssl_truststore_type => "JKS"
    ssl_truststore_password => "password"

    ssl_keystore_location => "/etc/logstash/security/client.keystore.jks"
    ssl_keystore_password => "password"
    ssl_keystore_type => "JKS"
    ssl_key_password => "password"

    ssl_endpoint_identification_algorithm => ""
  }
}

filter {
  # Extract env and service from topic as fallback
  grok {
    match => { "[@metadata][kafka][topic]" => "%{WORD:topic_env}\.%{GREEDYDATA:topic_service}-logs" }
  }

  mutate {
    # Temporarily move the fields from Logback customFields
    rename => { "service" => "app_service" }
    rename => { "environment" => "app_env" }

    # Restore standard names using the Logback values
    add_field => { "service" => "%{app_service}" }
    add_field => { "env" => "%{app_env}" }

    # Fallback to topic parsing if Logback fields are missing
    if ! [app_service] {
      add_field => { "service" => "%{topic_service}" }
    }
    if ! [app_env] {
      add_field => { "env" => "%{topic_env}" }
    }

    # Ensure they are strings (prevents arrays)
    convert => {
      "service" => "string"
      "env" => "string"
    }

    # Clean up temporary fields
    remove_field => [ "topic_env", "topic_service", "app_service", "app_env" ]
  }

  # Data stream routing â€“ reliable
  mutate {
    add_field => {
      "[data_stream][type]" => "logs"
      "[data_stream][dataset]" => "%{[service]}"
      "[data_stream][namespace]" => "%{[env]}"
    }
  }

  # Remove sensitive/unnecessary fields
  mutate {
    remove_field => [
      "kafka-pass", "ts-pwd", "ks-pwd",
      "ks-loc", "ts-loc", "kafka-host",
      "kafka-user", "kafka-topic", "tags",
      "[event][original]"
    ]
  }

  # Optional: hide data_stream hash from document
  mutate {
    remove_field => [ "data_stream" ]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "password"
    ssl_verification_mode => "none"

  # index config
  #  data_stream => false
  #  index => "%{[@metadata][kafka][topic]}-%{+YYYY.MM.dd}"

  # data_stream config
    data_stream => true
   # data_stream_type => "logs"
   # data_stream_dataset =>  "%{[service-name]}"
   # data_stream_namespace => "%{[environment]}"

#     manage_template => true
  }
   stdout { codec => rubydebug }
}