# logstash-kafka.conf
input {
  kafka {
    bootstrap_servers => "192.168.240.52:9093,192.168.240.34:9093"
    topics => ["logs-topic"] # Must match 'logging.kafka.topic'
    group_id => "logstash-group"
    auto_offset_reset => "earliest"
    codec => "json" # This is key! It parses the JSON logs sent by Spring Boot.


    # kafka security
    security_protocol => "SASL_SSL"
    sasl_mechanism => "SCRAM-SHA-512"

    #sasl_jaas_config => "org.apache.kafka.common.security.scram.ScramLoginModule required username=\"logstash\" password=\"logstash@2025\";"
    sasl_jaas_config => 'org.apache.kafka.common.security.scram.ScramLoginModule required username="logstash" password="logstash@2025";'
    ssl_truststore_location => "/etc/logstash/security/client.truststore.jks"
    ssl_truststore_password => "password"
    ssl_truststore_type => "PKCS12"

    ssl_keystore_location => "/etc/logstash/security/client.keystore.jks"
    ssl_keystore_password => "password"
    ssl_keystore_type => "PKCS12"
    ssl_key_password => "password"

    ssl_endpoint_identification_algorithm => ""
  }
}

filter {
  # No complex filtering needed since the log is alr    eady structured JSON.
  # Use this section only for enrichment or normalization if required.
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "password"
    ssl_verification_mode => "none"
    data_stream => false
    index => "mail-service-logs-%{+YYYY.MM.dd}"
    # Use the @timestamp field from the JSON log as the event timestamp
    manage_template => false
  }
}