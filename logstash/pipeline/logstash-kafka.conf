# logstash-kafka.conf
input {
  kafka {
    bootstrap_servers => "192.168.240.52:9093,192.168.240.34:9093,192.168.245.118:9093"
  #  bootstrap_servers => "kafka:29092"
    topics_pattern => ".*-logs" # subscribe to All service logs topics
   # topics => ["logs-topic"] # Must match 'logging.kafka.topic'
    group_id => "logs-consumer-group"
    auto_offset_reset => "earliest"
    codec => "json" # This is key! It parses the JSON logs sent by Spring Boot.
    decorate_events => true


    # kafka security
    security_protocol => "SASL_SSL"
    sasl_mechanism => "SCRAM-SHA-512"

    #sasl_jaas_config => "org.apache.kafka.common.security.scram.ScramLoginModule required username=\"logstash\" password=\"logstash@2025\";"
    sasl_jaas_config => 'org.apache.kafka.common.security.scram.ScramLoginModule required username="logstash" password="logstash@2025";'
    ssl_truststore_location => "/etc/logstash/security/client.truststore.jks"
    ssl_truststore_type => "JKS"
    ssl_truststore_password => "password"

    ssl_keystore_location => "/etc/logstash/security/client.keystore.jks"
    ssl_keystore_password => "password"
    ssl_keystore_type => "JKS"
    ssl_key_password => "password"

    ssl_endpoint_identification_algorithm => ""
  }
}

filter {
  # 1. Extract metadata from topic as fallback
  grok {
    match => { "[@metadata][kafka][topic]" => "%{WORD:topic_env}\.%{GREEDYDATA:topic_service}-logs" }
  }

  # 2. Handle Service Name
  # If Spring Boot didn't send 'service', use the topic metadata
  if ![service] {
    mutate { add_field => { "service" => "%{topic_service}" } }
  }

  # 3. Handle Environment Name
  # If Spring Boot didn't send 'environment', use topic metadata
  # Then map it to 'env' for the data stream
  if [environment] {
    mutate { add_field => { "env" => "%{environment}" } }
  } else {
    mutate { add_field => { "env" => "%{topic_env}" } }
  }
 # Data stream routing â€“ reliable
  mutate {
    add_field => {
      "[data_stream][type]" => "logs"
      "[data_stream][dataset]" => "%{[service]}"
      "[data_stream][namespace]" => "%{[env]}"
    }
  }
  # 4. Final Cleanup of Sensitive and Unnecessary fields


  # Remove sensitive/unnecessary fields
    mutate {
      remove_field => [
        "kafka-pass", "ts-pwd", "ks-pwd",
        "ks-loc", "ts-loc", "kafka-host",
        "kafka-user", "kafka-topic", "tags",
        "[event][original]"
      ]
    }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "password"
    ssl_verification_mode => "none"

    data_stream => true
  }

  # Useful for debugging in 'docker logs'
  stdout { codec => rubydebug }
}